{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b630c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, contextlib, gc\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from gemma.config import get_model_config\n",
    "from gemma.model import GemmaModel, GemmaForCausalLM, gemma_config\n",
    "from gemma.siglip_vision.config import get_siglip_vision_model_config\n",
    "\n",
    "model_checkpoint_path: Path = Path(\"gemma/gemma-3-1b-pt-checkpoint/model.ckpt\")\n",
    "tokenizer_path: Path = Path(\"gemma/gemma-3-1b-pt-checkpoint/tokenizer.model\")\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.Load(model_file=tokenizer_path.__str__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe59d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (embedder): Embedding()\n",
       "  (model): GemmaModel(\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (qkv_proj): Linear()\n",
       "          (o_proj): Linear()\n",
       "          (query_norm): RMSNorm()\n",
       "          (key_norm): RMSNorm()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear()\n",
       "          (up_proj): Linear()\n",
       "          (down_proj): Linear()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (pre_feedforward_layernorm): RMSNorm()\n",
       "        (post_feedforward_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (sampler): Sampler()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = gemma_config.get_config_for_1b(\"bfloat16\")\n",
    "model_config.tokenizer = tokenizer_path.__str__()\n",
    "model = GemmaForCausalLM(model_config)\n",
    "model.load_state_dict(\n",
    "    torch.load(model_checkpoint_path, weights_only=False)[\"model_state_dict\"]\n",
    ")\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
